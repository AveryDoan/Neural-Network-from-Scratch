{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own Neural Network from Scratch\n",
    "\n",
    "Welcome! This notebook demonstrates how to build and train a neural network using a **modular architecture**. \n",
    "\n",
    "### Why Modular?\n",
    "In real-world frameworks like PyTorch or TensorFlow, neural networks are built by stacking independent components (Layers). This modularity allows us to:\n",
    "1. **Understand** each part in isolation (Linear, Activation, Loss).\n",
    "2. **Customize** the architecture easily (add more layers, change sizes).\n",
    "3. **Debug** specific components during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neural_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3090972487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neural_network'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from neural_network import Linear, ReLU, Sequential, CrossEntropyLoss, SGD\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data(n_samples=2000, n_features=4, centers=3):\n",
    "    \"\"\"Generates synthetic data (blobs) for classification.\"\"\"\n",
    "    X, y = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features, random_state=42)\n",
    "    y_oh = np.eye(centers)[y] # One-hot encode targets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test, centers\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_classes = load_data()\n",
    "print(f\"Data loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Design Your Network\n",
    "\n",
    "Here you can customize the **important parameters**:\n",
    "- `hidden_dim`: Number of neurons in the hidden layer (try 8, 32, or 128).\n",
    "- `learning_rate`: How fast the model learns (try 0.1, 0.01, or 0.001).\n",
    "- `regularization`: Prevents overfitting (try 1e-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CUSTOMIZE THESE PARAMETERS ---\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.1\n",
    "REGULARIZATION = 1e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "# ----------------------------------\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = num_classes\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Linear(input_dim, HIDDEN_DIM),\n",
    "    ReLU(),\n",
    "    Linear(HIDDEN_DIM, output_dim)\n",
    "])\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = SGD(model.layers, lr=LEARNING_RATE, reg=REGULARIZATION)\n",
    "\n",
    "print(f\"Model built with {HIDDEN_DIM} hidden units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Training Loop\n",
    "\n",
    "This is where the \"learning\" happens in 3 steps:\n",
    "1. **Forward**: Data passes through layers to get predictions.\n",
    "2. **Loss**: Measure how far off we are from the truth.\n",
    "3. **Backward**: Calculate gradients and use SGD to update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, y):\n",
    "    scores = model.forward(X)\n",
    "    preds = np.argmax(scores, axis=1)\n",
    "    targets = np.argmax(y, axis=1)\n",
    "    return np.mean(preds == targets)\n",
    "\n",
    "losses = []\n",
    "N = X_train.shape[0]\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Shuffle data for each epoch\n",
    "    indices = np.random.permutation(N)\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i in range(0, N, BATCH_SIZE):\n",
    "        xb = X_shuffled[i:i+BATCH_SIZE]\n",
    "        yb = y_shuffled[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        scores = model.forward(xb)\n",
    "        \n",
    "        # 2. Compute Loss\n",
    "        loss = loss_fn.loss(scores, yb)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # 3. Backward pass (Gradients)\n",
    "        dloss = loss_fn.backward(yb)\n",
    "        model.backward(dloss)\n",
    "        \n",
    "        # 4. Optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = epoch_loss / (N / BATCH_SIZE)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        train_acc = calculate_accuracy(model, X_train, y_train)\n",
    "        test_acc = calculate_accuracy(model, X_test, y_test)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Predictions\n",
    "\n",
    "Let's see how our model performs visually by plotting the decision boundaries (if possible) or just checking a few random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = np.random.randint(0, X_test.shape[0], 5)\n",
    "for idx in sample_idx:\n",
    "    x = X_test[idx:idx+1]\n",
    "    y = y_test[idx]\n",
    "    scores = model.forward(x)\n",
    "    pred = np.argmax(scores)\n",
    "    actual = np.argmax(y)\n",
    "    print(f\"Sample {idx}: Prediction={pred}, Actual={actual} {'✅' if pred == actual else '❌'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
