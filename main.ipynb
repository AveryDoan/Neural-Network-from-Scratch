{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 4), (1600, 3))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml, make_blobs\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data(use_synthetic=False, n_samples=5000):\n",
    "    if use_synthetic:\n",
    "        X, y = make_blobs(n_samples=2000, centers=3, n_features=4, random_state=0)\n",
    "        num_classes = 3\n",
    "    else:\n",
    "        mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "        X, y = mnist[\"data\"] / 255.0, mnist[\"target\"].astype(int)\n",
    "        num_classes = 10\n",
    "        X, y = X[:n_samples], y[:n_samples]  # speed\n",
    "    y_oh = np.eye(num_classes)[y]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test, num_classes\n",
    "\n",
    "X_train, X_test, y_train, y_test, num_classes = load_data(use_synthetic=True)\n",
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.1, reg=1e-4):\n",
    "        self.lr, self.reg = lr, reg\n",
    "        self.params = {\n",
    "            \"W1\": 0.01 * np.random.randn(input_dim, hidden_dim),\n",
    "            \"b1\": np.zeros((1, hidden_dim)),\n",
    "            \"W2\": 0.01 * np.random.randn(hidden_dim, output_dim),\n",
    "            \"b2\": np.zeros((1, output_dim)),\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        W1, b1, W2, b2 = self.params.values()\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        scores = a1 @ W2 + b2\n",
    "        scores -= scores.max(axis=1, keepdims=True)\n",
    "        exp = np.exp(scores)\n",
    "        probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "        cache = (X, z1, a1, probs)\n",
    "        return probs, cache\n",
    "\n",
    "    def loss(self, probs, y_true):\n",
    "        N = y_true.shape[0]\n",
    "        log_likelihood = -np.log(probs[y_true.astype(bool)])\n",
    "        data_loss = log_likelihood.mean()\n",
    "        reg_loss = 0.5 * self.reg * sum((p**2).sum() for k, p in self.params.items() if k.startswith(\"W\"))\n",
    "        return data_loss + reg_loss\n",
    "\n",
    "    def backward(self, cache, probs, y_true):\n",
    "        X, z1, a1, _ = cache\n",
    "        N = X.shape[0]\n",
    "        grads = {}\n",
    "        dscores = (probs - y_true) / N\n",
    "        grads[\"W2\"] = a1.T @ dscores + self.reg * self.params[\"W2\"]\n",
    "        grads[\"b2\"] = dscores.sum(axis=0, keepdims=True)\n",
    "        da1 = dscores @ self.params[\"W2\"].T\n",
    "        dz1 = da1 * (z1 > 0)\n",
    "        grads[\"W1\"] = X.T @ dz1 + self.reg * self.params[\"W1\"]\n",
    "        grads[\"b1\"] = dz1.sum(axis=0, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def step(self, grads):\n",
    "        for k in self.params:\n",
    "            self.params[k] -= self.lr * grads[k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_acc=0.999, val_acc=1.000\n",
      "epoch 2: train_acc=1.000, val_acc=1.000\n",
      "epoch 4: train_acc=1.000, val_acc=1.000\n",
      "epoch 6: train_acc=1.000, val_acc=1.000\n",
      "epoch 8: train_acc=1.000, val_acc=1.000\n",
      "epoch 10: train_acc=1.000, val_acc=1.000\n",
      "epoch 12: train_acc=1.000, val_acc=1.000\n",
      "epoch 14: train_acc=1.000, val_acc=1.000\n",
      "epoch 16: train_acc=1.000, val_acc=1.000\n",
      "epoch 18: train_acc=1.000, val_acc=1.000\n",
      "epoch 20: train_acc=1.000, val_acc=1.000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(model, X, y):\n",
    "    probs, _ = model.forward(X)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    targets = y.argmax(axis=1)\n",
    "    return (preds == targets).mean()\n",
    "\n",
    "def train(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=64):\n",
    "    N = X_train.shape[0]\n",
    "    for epoch in range(1, epochs+1):\n",
    "        idx = np.random.permutation(N)\n",
    "        X_train, y_train = X_train[idx], y_train[idx]\n",
    "        for i in range(0, N, batch_size):\n",
    "            xb, yb = X_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "            probs, cache = model.forward(xb)\n",
    "            grads = model.backward(cache, probs, yb)\n",
    "            model.step(grads)\n",
    "        if epoch % 2 == 0 or epoch == 1:\n",
    "            train_acc = accuracy(model, X_train, y_train)\n",
    "            val_acc = accuracy(model, X_val, y_val)\n",
    "            print(f\"epoch {epoch}: train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=128, output_dim=num_classes, lr=0.1)\n",
    "train(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad diff: 3.0785598113158576e-10\n"
     ]
    }
   ],
   "source": [
    "def numerical_grad(f, x, eps=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old = x[idx]\n",
    "        x[idx] = old + eps\n",
    "        plus = f(x)\n",
    "        x[idx] = old - eps\n",
    "        minus = f(x)\n",
    "        x[idx] = old\n",
    "        grad[idx] = (plus - minus) / (2 * eps)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "# Check W1 on a very small batch\n",
    "mini_X, mini_y = X_train[:5], y_train[:5]\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=5, output_dim=num_classes, lr=0.1, reg=0.0)\n",
    "probs, cache = model.forward(mini_X)\n",
    "def loss_W1(W1):\n",
    "    model.params[\"W1\"] = W1\n",
    "    p, c = model.forward(mini_X)\n",
    "    return model.loss(p, mini_y)\n",
    "grads = model.backward(cache, probs, mini_y)\n",
    "num_g = numerical_grad(loss_W1, model.params[\"W1\"].copy())\n",
    "print(\"grad diff:\", np.linalg.norm(grads[\"W1\"] - num_g) / (np.linalg.norm(grads[\"W1\"]) + np.linalg.norm(num_g)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
